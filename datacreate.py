import google.generativeai as genai
import os
import json
import time
import sys
import random # randomãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¿½åŠ 

# --- ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒãƒ¼é–¢æ•° ---
def countdown_timer(seconds):
    """æŒ‡å®šã•ã‚ŒãŸç§’æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³è¡¨ç¤ºã™ã‚‹é–¢æ•°"""
    try:
        for i in range(seconds, 0, -1):
            mins, secs = divmod(i, 60)
            timer_display = f"    ...æ¬¡ã®å‡¦ç†ã¾ã§ã‚ã¨ {mins:02d}åˆ†{secs:02d}ç§’..."
            sys.stdout.write(timer_display)
            sys.stdout.flush()
            time.sleep(1)
            sys.stdout.write('\r' + ' ' * len(timer_display) + '\r')
    except KeyboardInterrupt:
        print("\nã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
        raise
    print("\n    ...å¾…æ©Ÿå®Œäº†ã€‚å‡¦ç†ã‚’å†é–‹ã—ã¾ã™...")

# APIã‚­ãƒ¼è¨­å®š
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

model_name = "models/gemini-2.5-flash"
output_file = "data.json"
questions_file = "questions.txt"

# ãƒªãƒˆãƒ©ã‚¤è¨­å®š
RETRY_WAIT_SECONDS = 30
MAX_RETRIES = 3

# å‹•çš„å¾…æ©Ÿæ™‚é–“èª¿æ•´ã®è¨­å®š
MIN_WAIT_TIME = 180  # æœ€å°å¾…æ©Ÿæ™‚é–“ (3åˆ†)
MAX_WAIT_TIME = 1800 # æœ€å¤§å¾…æ©Ÿæ™‚é–“ (30åˆ†)
WAIT_TIME_DECREASE_FACTOR = 0.9 # æˆåŠŸæ™‚ã«å¾…æ©Ÿæ™‚é–“ã‚’æ¸›ã‚‰ã™ä¿‚æ•°
WAIT_TIME_INCREASE_FACTOR = 1.5 # å¤±æ•—æ™‚ã«å¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™ä¿‚æ•°

print("=== Geminiãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ (å†é–‹ãƒ»ã‚¿ã‚¤ãƒãƒ¼ãƒ»ãƒªãƒˆãƒ©ã‚¤ãƒ»å‹•çš„å¾…æ©Ÿæ©Ÿèƒ½ä»˜ã) ===")

# --- 1. æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ ---
existing_results = []
processed_prompts = set()
if os.path.exists(output_file):
    try:
        with open(output_file, "r", encoding="utf-8") as f:
            if os.path.getsize(output_file) > 0:
                existing_results = json.load(f)
                if not isinstance(existing_results, list):
                    print(f"è­¦å‘Š: '{output_file}' ã®å½¢å¼ãŒä¸æ­£ãªãŸã‚ã€åˆæœŸåŒ–ã—ã¾ã™ã€‚")
                    existing_results = []
            
            for record in existing_results:
                if 'prompt' in record:
                    processed_prompts.add(record['prompt'])
        if existing_results:
            print(f"'{output_file}' ã‹ã‚‰ {len(existing_results)} ä»¶ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except (json.JSONDecodeError, IOError) as e:
        print(f"è­¦å‘Š: '{output_file}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚ ({e})")
        existing_results = []
        processed_prompts = set()

# --- 2. è³ªå•ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€ ---
try:
    with open(questions_file, "r", encoding="utf-8") as f:
        all_questions = [line.strip() for line in f if line.strip()]
except FileNotFoundError:
    print(f"ã‚¨ãƒ©ãƒ¼: '{questions_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    exit()

# --- 3. æœªå‡¦ç†ã®è³ªå•ã‚’ç‰¹å®šã™ã‚‹ ---
questions_to_process = [q for q in all_questions if q not in processed_prompts]

if not questions_to_process:
    print("ã™ã¹ã¦ã®è³ªå•ã®å‡¦ç†ãŒå®Œäº†ã—ã¦ã„ã¾ã™ã€‚")
    exit()

print(f"å…¨ {len(all_questions)} å•ä¸­ã€æœªå‡¦ç†ã® {len(questions_to_process)} å•ã‚’å‡¦ç†ã—ã¾ã™ã€‚")

# --- 4. æœªå‡¦ç†ã®è³ªå•ã‚’å‡¦ç†ã—ã€çµæœã‚’è¿½è¨˜ ---
newly_added_count = 0
total_to_process = len(questions_to_process)

# å‹•çš„å¾…æ©Ÿæ™‚é–“èª¿æ•´ã®çŠ¶æ…‹å¤‰æ•°
current_wait_time = MIN_WAIT_TIME
consecutive_successes = 0
consecutive_failures = 0

for count, prompt in enumerate(questions_to_process, 1):
    model = genai.GenerativeModel(model_name)
    print(f"[{len(existing_results) + count}/{len(all_questions)}] ğŸ§  '{prompt[:30]}...' ã‚’ç”Ÿæˆä¸­...")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ–‡å­—æ•°åˆ¶é™ã®æŒ‡ç¤ºã‚’å¼·åŒ–
    modified_prompt = f"{prompt}ã€‚å›ç­”ã¯å³å¯†ã«100æ–‡å­—ä»¥å†…ã«å®Œçµã•ã›ã¦ãã ã•ã„ã€‚"
    
    # è³ªå•å†…å®¹ã«å¿œã˜ã¦max_output_tokensã‚’å‹•çš„ã«è¨­å®š
    max_tokens_for_this_request = 2048 if "ã‚³ãƒ¼ãƒ‰" in prompt or "python" in prompt.lower() else 1024

    response = None # responseã‚’åˆæœŸåŒ–
    for attempt in range(MAX_RETRIES):
        try:
            response = model.generate_content(
                modified_prompt, # ä¿®æ­£ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=max_tokens_for_this_request # å‹•çš„ã«è¨­å®šã—ãŸå€¤ã‚’ä½¿ç”¨
                ),
                request_options={"timeout": 180} # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’180ç§’ã«å¤‰æ›´
            )
            break # æˆåŠŸã—ãŸã‚‰ãƒªãƒˆãƒ©ã‚¤ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                print(f"    âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}ã€‚{RETRY_WAIT_SECONDS}ç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™â€¦ (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})")
                countdown_timer(RETRY_WAIT_SECONDS) # ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿã‚‚ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³è¡¨ç¤º
            else:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}ã€‚æœ€å¤§è©¦è¡Œå›æ•° ({MAX_RETRIES}) ã‚’è¶…ãˆã¾ã—ãŸã€‚ã“ã®è³ªå•ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                response = None # æœ€çµ‚çš„ã«å¤±æ•—ã—ãŸå ´åˆã¯responseã‚’Noneã«ã™ã‚‹

    # ãƒªãƒˆãƒ©ã‚¤æˆåŠŸã—ãŸå ´åˆã€ã¾ãŸã¯æœ€çµ‚çš„ã«å¤±æ•—ã—ãŸå ´åˆã®å‡¦ç†
    try:
        if response:
            if response.candidates and response.candidates[0].content.parts:
                answer = response.text.strip()
                record = {"prompt": prompt, "response": answer}
                existing_results.append(record)
                newly_added_count += 1
                print(f"    âœ… ç”Ÿæˆå®Œäº†")

                # æˆåŠŸã—ãŸå ´åˆã®ã¿å¾…æ©Ÿå‡¦ç†ã‚’å®Ÿè¡Œ
                if count < total_to_process:
                    # æˆåŠŸæ™‚ã®å¾…æ©Ÿæ™‚é–“èª¿æ•´
                    consecutive_successes += 1
                    consecutive_failures = 0
                    current_wait_time = max(MIN_WAIT_TIME, current_wait_time * WAIT_TIME_DECREASE_FACTOR)
                    countdown_timer(int(current_wait_time)) 
            else:
                reason = response.candidates[0].finish_reason.name if response.candidates else 'N/A'
                print(f"    âš ï¸ å¿œç­”ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ç†ç”±: {reason}")
                # å¿œç­”ãŒãªã„å ´åˆã‚‚ã€æ¬¡ã®å‡¦ç†ã¾ã§å°‘ã—å¾…ã¤
                if count < total_to_process:
                    # å¤±æ•—æ™‚ã®å¾…æ©Ÿæ™‚é–“èª¿æ•´
                    consecutive_failures += 1
                    consecutive_successes = 0
                    current_wait_time = min(MAX_WAIT_TIME, current_wait_time * WAIT_TIME_INCREASE_FACTOR)
                    countdown_timer(int(current_wait_time)) # å¤±æ•—æ™‚ã¯é•·ã‚ã®å¾…æ©Ÿ
        else:
            print(f"    âš ï¸ ã“ã®è³ªå•ã®ç”Ÿæˆã¯æœ€çµ‚çš„ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            # å¤±æ•—ã—ãŸå ´åˆã‚‚ã€æ¬¡ã®å‡¦ç†ã¾ã§å°‘ã—å¾…ã¤
            if count < total_to_process:
                # å¤±æ•—æ™‚ã®å¾…æ©Ÿæ™‚é–“èª¿æ•´
                consecutive_failures += 1
                consecutive_successes = 0
                current_wait_time = min(MAX_WAIT_TIME, current_wait_time * WAIT_TIME_INCREASE_FACTOR)
                countdown_timer(int(current_wait_time)) # å¤±æ•—æ™‚ã¯é•·ã‚ã®å¾…æ©Ÿ

    except KeyboardInterrupt:
        print("\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚ç¾åœ¨ã¾ã§ã®çµæœã‚’ä¿å­˜ã—ã¾ã™ã€‚")
        break
    except Exception as e:
        print(f"    âš ï¸ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        print("å‡¦ç†ã‚’ä¸­æ–­ã—ã€ç¾åœ¨ã¾ã§ã®çµæœã‚’ä¿å­˜ã—ã¾ã™ã€‚")
        break
    finally:
        # ãƒ«ãƒ¼ãƒ—ã®åå¾©ã®æœ€å¾Œã«ã€æˆåŠŸãƒ»å¤±æ•—ãƒ»ä¸­æ–­(breakå‰)ã«é–¢ã‚ã‚‰ãšå¿…ãšä¿å­˜
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(existing_results, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"    âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

print("\n" + "="*30)
if newly_added_count > 0:
    print(f"ğŸ‰ ä»Šå› {newly_added_count} ä»¶ã®æ–°ã—ã„çµæœã‚’è¿½åŠ ã—ã€åˆè¨ˆ {len(existing_results)} ä»¶ã‚’ '{output_file}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
else:
    print("æ–°ã—ã„çµæœã¯è¿½åŠ ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
print("="*30)